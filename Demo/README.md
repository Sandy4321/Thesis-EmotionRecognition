# Demo of Emotion Recognition

A demo of predicting binary Arousal and Valence from speech recording and transcript

All codes written in Python 2.7.13.

## Demo.py
This script reads in the features extracted by FeatExtract.py and makes predictions using the pre-trained models (these models are generated by TrainModel_A.py and TrainModel_V.py and are saved in the folder /pre-trained). keras, pandas, matplotlib, and h5py are required and you might need to pip install them. The pre-trained models will give a binary Arousal prediction and a binary Valence prediction, and emoji are used to illustrate these predictions. The test data included should be classified as having positive Arousal and positive Valence. Here I use Pusheen the cat stickers in the folder /PusheenEmotion, but you can switch to a more serious set of output included in the foler /BoringEmoji

## FeatExtract.py
This script extracts CSA lexcial features from transcripts (word + word timing) and eGeMAPS acoustic features from wav recording. The longer your test recording is, the longer feature extraction will take. The test recording was about 30 minutes long and containing 122 utterances, and feature extraction took about 30 seconds. At the moment I cut the transcripts into segments of 5 seconds as utterances. I cannot provide the test recording and transcript due to data privacy concerns, but I have provided the features extracted so you can run Demo.py directly. Emotion of the original recording is positive Arousal and positive Valence. nltk, ffmpeg, and opensmile are required for feature extraction. The affective lexicon dictionary for extracting CSA features is included in the foler /pretrained

## TrainModel_A.py and TrainModel_V.py
These scripts are used to train the Arousal and Valence binary classification models. At the moment I use utterance-level AVEC2012 & IEMOCAP as the training data. There are two feature sets used: eGeMAPS for audio and CSA for lexical. Simple FL fusion is applied. You can use other datasets. You can also use other features or recognition models, just remember to adapt FeatExtract.py and Demo.py accordingly.
