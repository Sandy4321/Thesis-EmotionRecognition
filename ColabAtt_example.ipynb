{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CC_ColabAtt_Both_I_FL_P.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2QWu/fw4zKfr2QcciaC+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianleimin/Thesis-EmotionRecognition/blob/master/ColabAtt_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zisrGCcZUkdU",
        "colab_type": "code",
        "outputId": "73935e89-2789-4534-ef16-d37e0063a475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Attention-BLSTM emotion classification (Arousal)\n",
        "# Use GP+DIS-NV features, train on both databases and test on a subset of IEMOCAP\n",
        "\n",
        "'''\n",
        "To prevent Colab from time out, go to the google Colab console (ctrl+shift+i) and type:\n",
        "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
        "\n",
        "Don't exit the console until you get \"Working\" as the output in the console window. \n",
        "It would keep on clicking the page and prevent it from disconnecting.\n",
        "Make sure you dont run anything for more than 12 hrs on Colab!\n",
        "'''\n",
        "\n",
        "# import the required modules\n",
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize,LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Attention implemented by https://github.com/CyberZHG/keras-self-attention\n",
        "!pip install keras-self-attention \n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "# define variables\n",
        "# parameters to be investigated in grid seearch\n",
        "time_steps = [2,4,8]  # input history to include, candidates: [2,4,8]\n",
        "lstm_sizes = [[16,8,4],[32,16,8],[64,32,16]] # number of neurons in the BLSTM layers, candidates: [[16,8,4],[32,16,8],[64,32,16]]\n",
        "attention_widths = [8,16,32] # width of the local context for the attention layer, candidates: [8,16,32]\n",
        "# other parameters\n",
        "batch_size = 32 # for estimating error gradient\n",
        "nb_features = 8 # number of features\n",
        "nb_class = 3 # number of classes\n",
        "nb_epoch = 1000 # number of total epochs to train the model\n",
        "\n",
        "# optimization function\n",
        "opt_func = Adamax(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
        "# to prevent over-fitting\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=50)\n",
        "\n",
        "# data files\n",
        "# access data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_log = '/content/drive/My Drive/Colab/TACoutputs/CC_Att_Both_I_GP+DN_A_log.txt'\n",
        "file_pred = '/content/drive/My Drive/Colab/TACoutputs/CC_Att_Both_I_GP+DN_A_pred.txt'\n",
        "file_emo_tst = '/content/drive/My Drive/Colab/TACdata/IEMOCAP_emo.csv'\n",
        "file_feat_tst = '/content/drive/My Drive/Colab/TACdata/IEMOCAP_GP+DN.csv'\n",
        "file_emo_trn = '/content/drive/My Drive/Colab/TACdata/utt_AVEC_emo.csv'\n",
        "file_feat_trn = '/content/drive/My Drive/Colab/TACdata/utt_AVEC_GP+DN.csv'\n",
        "\n",
        "# turn off the warnings, be careful when use this\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# reshape panda.DataFrame to Keras style: (batch_size, time_step, nb_features)\n",
        "def reshape_data(data, n_prev):\n",
        "    docX = []\n",
        "    for i in range(len(data)):\n",
        "        if i < (len(data)-n_prev):\n",
        "            docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
        "        else: # the frames in the last window use the same context\n",
        "            docX.append(data.iloc[(len(data)-n_prev):len(data)].as_matrix())\n",
        "    alsX = np.array(docX)\n",
        "    return alsX\n",
        "\n",
        "# define the BLSTM model with attention\n",
        "def attBLSTM(lstm_size, attention_width, nb_class, opt_func):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[0], return_sequences=True))) # BLSTM layer 1\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[1], return_sequences=True))) # BLSTM layer 2\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[2], return_sequences=True))) # BLSTM layer 3\n",
        "    model.add(SeqSelfAttention(attention_width=attention_width, attention_activation='sigmoid')) # attention layer\n",
        "    model.add(Dense(units=nb_class, activation='softmax')) # output layer, predict emotion dimensions seperately\n",
        "    return model\n",
        "\n",
        "# read in data\n",
        "trn_feat = pd.read_csv(file_feat_trn, header=None)\n",
        "tst_feat = pd.read_csv(file_feat_tst, header=None)\n",
        "# normalize features\n",
        "trn_feat = (trn_feat - trn_feat.min())/(trn_feat.max() - trn_feat.min())\n",
        "tst_feat = (tst_feat - tst_feat.min())/(tst_feat.max() - tst_feat.min())\n",
        "# combine the two databases\n",
        "all_feat = pd.concat([trn_feat,tst_feat])\n",
        "trn_emo_raw = pd.read_csv(file_emo_trn, header=None, usecols=[0])\n",
        "trn_emo_raw = trn_emo_raw.values\n",
        "# one-hot encoding of the classes\n",
        "trn_emo = []\n",
        "for label in trn_emo_raw:\n",
        "    if label == 0:\n",
        "        converted_label = [0,1,0] # medium\n",
        "    elif label == 1:\n",
        "        converted_label = [0,0,1] # high\n",
        "    else:\n",
        "        converted_label = [1,0,0] # low\n",
        "    trn_emo.append(converted_label)\n",
        "y_train = np.asarray(trn_emo)\n",
        "y_train_df = pd.DataFrame(y_train, index=None)\n",
        "tst_emo_raw = pd.read_csv(file_emo_tst, header=None, usecols=[0])\n",
        "tst_emo_raw = tst_emo_raw.values\n",
        "# one-hot encoding of the classes\n",
        "tst_emo = []\n",
        "for label in tst_emo_raw:\n",
        "    if label == 0:\n",
        "        converted_label = [1,0,0] # low\n",
        "    elif label == 1:\n",
        "        converted_label = [0,1,0] # medium\n",
        "    else:\n",
        "        converted_label = [0,0,1] # high\n",
        "    tst_emo.append(converted_label)\n",
        "y_test = np.asarray(tst_emo)\n",
        "y_test_df = pd.DataFrame(y_test, index=None)\n",
        "# combine the two databases\n",
        "all_emo_raw = pd.concat([y_train_df,y_test_df])\n",
        "\n",
        "# Grid search for best parameters\n",
        "para_list = []\n",
        "tst_pred_list = []\n",
        "f1_list = []\n",
        "count = 1\n",
        "for time_step in time_steps:\n",
        "    X = reshape_data(all_feat, time_step) # pad feature data\n",
        "    y = reshape_data(all_emo_raw, time_step) # pad label data\n",
        "    # save a subset of IEMOCAP for testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, shuffle=False)\n",
        "    for lstm_size in lstm_sizes:\n",
        "        for attention_width in attention_widths:\n",
        "            para_list.append([time_step, lstm_size, attention_width]) # save parameter set\n",
        "            print('\\n================================ No. %s of 27 ========================================' % count)\n",
        "            print('\\nParameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
        "                  % (time_step, lstm_size, attention_width))\n",
        "            # build model with given parameters\n",
        "            model = attBLSTM(lstm_size, attention_width, nb_class, opt_func)\n",
        "            # compile the model\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
        "            # training the model\n",
        "            model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, \n",
        "                      validation_split=0.05, callbacks=[early_stopping], verbose=0)\n",
        "            # evaluation\n",
        "            model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "            # save predictions\n",
        "            tst_pred = model.predict(X_test)\n",
        "            tst_pred_list.append(tst_pred) # save predictions\n",
        "\n",
        "            # print confusion matrix\n",
        "            y_test_non_category = [ np.argmax(t[0]) for t in y_test ]\n",
        "            y_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
        "            print('Confusion Matrix on test set')\n",
        "            print(confusion_matrix(y_test_non_category, y_predict_non_category))\n",
        "            tst_f1 = f1_score(y_test_non_category, y_predict_non_category, average='weighted')\n",
        "            f1_list.append(tst_f1) # save f1 score\n",
        "            print('Weighted F1-score on test set:', tst_f1)\n",
        "            # print grid search log\n",
        "            with open(file_log, 'a') as logfile:\n",
        "                logfile.write('\\n================================ No. %s of 27 ========================================' % count)\n",
        "                logfile.write('F1 = %s; Parameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
        "                              % (tst_f1, time_step, lstm_size, attention_width))\n",
        "                logfile.write('Confusion Matrix on test set')\n",
        "                logfile.write(confusion_matrix(y_test_non_category, y_predict_non_category))           \n",
        "            count = count + 1\n",
        "\n",
        "# save the best parameter set and its predictions\n",
        "best = f1_list.index(max(f1_list)) # find the highest F1 score\n",
        "result = f1_list[best]\n",
        "para = para_list[best]\n",
        "prediction = tst_pred_list[best]\n",
        "with open(file_pred, 'a') as predfile:\n",
        "    predfile.write('F1 = %s; Parameters: time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
        "                   % (result, para[0], para[1], para[2]))\n",
        "    for pred in prediction:\n",
        "        indi_pred = []\n",
        "        indi_pred = pred[0] # reform the seq prediction to individual samples\n",
        "        row = ', '.join(map(str, indi_pred))\n",
        "        predfile.write('%s\\n' % row)\n",
        "\n",
        "print('\\nDone!')"
      ],
      "execution_count": 0,
    }
  ]
}
