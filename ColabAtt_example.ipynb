{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CC_ColabAtt_Both_I_FL_P.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2QWu/fw4zKfr2QcciaC+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianleimin/Thesis-EmotionRecognition/blob/master/ColabAtt_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zisrGCcZUkdU",
        "colab_type": "code",
        "outputId": "73935e89-2789-4534-ef16-d37e0063a475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Attention-BLSTM emotion classification (Arousal)\n",
        "# Use GP+DIS-NV features, train on both databases and test on a subset of IEMOCAP\n",
        "\n",
        "'''\n",
        "To prevent Colab from time out, go to the google Colab console (ctrl+shift+i) and type:\n",
        "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
        "\n",
        "Don't exit the console until you get \"Working\" as the output in the console window. \n",
        "It would keep on clicking the page and prevent it from disconnecting.\n",
        "Make sure you dont run anything for more than 12 hrs on Colab!\n",
        "'''\n",
        "\n",
        "# import the required modules\n",
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize,LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Attention implemented by https://github.com/CyberZHG/keras-self-attention\n",
        "!pip install keras-self-attention \n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "# define variables\n",
        "# parameters to be investigated in grid seearch\n",
        "time_steps = [2,4,8]  # input history to include, candidates: [2,4,8]\n",
        "lstm_sizes = [[16,8,4],[32,16,8],[64,32,16]] # number of neurons in the BLSTM layers, candidates: [[16,8,4],[32,16,8],[64,32,16]]\n",
        "attention_widths = [8,16,32] # width of the local context for the attention layer, candidates: [8,16,32]\n",
        "# other parameters\n",
        "batch_size = 32 # for estimating error gradient\n",
        "nb_features = 8 # number of features\n",
        "nb_class = 3 # number of classes\n",
        "nb_epoch = 1000 # number of total epochs to train the model\n",
        "\n",
        "# optimization function\n",
        "opt_func = Adamax(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
        "# to prevent over-fitting\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=50)\n",
        "\n",
        "# data files\n",
        "# access data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_log = '/content/drive/My Drive/Colab/TACoutputs/CC_Att_Both_I_GP+DN_A_log.txt'\n",
        "file_pred = '/content/drive/My Drive/Colab/TACoutputs/CC_Att_Both_I_GP+DN_A_pred.txt'\n",
        "file_emo_tst = '/content/drive/My Drive/Colab/TACdata/IEMOCAP_emo.csv'\n",
        "file_feat_tst = '/content/drive/My Drive/Colab/TACdata/IEMOCAP_GP+DN.csv'\n",
        "file_emo_trn = '/content/drive/My Drive/Colab/TACdata/utt_AVEC_emo.csv'\n",
        "file_feat_trn = '/content/drive/My Drive/Colab/TACdata/utt_AVEC_GP+DN.csv'\n",
        "\n",
        "# turn off the warnings, be careful when use this\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# reshape panda.DataFrame to Keras style: (batch_size, time_step, nb_features)\n",
        "def reshape_data(data, n_prev):\n",
        "    docX = []\n",
        "    for i in range(len(data)):\n",
        "        if i < (len(data)-n_prev):\n",
        "            docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
        "        else: # the frames in the last window use the same context\n",
        "            docX.append(data.iloc[(len(data)-n_prev):len(data)].as_matrix())\n",
        "    alsX = np.array(docX)\n",
        "    return alsX\n",
        "\n",
        "# define the BLSTM model with attention\n",
        "def attBLSTM(lstm_size, attention_width, nb_class, opt_func):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[0], return_sequences=True))) # BLSTM layer 1\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[1], return_sequences=True))) # BLSTM layer 2\n",
        "    model.add(Bidirectional(LSTM(units=lstm_size[2], return_sequences=True))) # BLSTM layer 3\n",
        "    model.add(SeqSelfAttention(attention_width=attention_width, attention_activation='sigmoid')) # attention layer\n",
        "    model.add(Dense(units=nb_class, activation='softmax')) # output layer, predict emotion dimensions seperately\n",
        "    return model\n",
        "\n",
        "# read in data\n",
        "trn_feat = pd.read_csv(file_feat_trn, header=None)\n",
        "tst_feat = pd.read_csv(file_feat_tst, header=None)\n",
        "# normalize features\n",
        "trn_feat = (trn_feat - trn_feat.min())/(trn_feat.max() - trn_feat.min())\n",
        "tst_feat = (tst_feat - tst_feat.min())/(tst_feat.max() - tst_feat.min())\n",
        "# combine the two databases\n",
        "all_feat = pd.concat([trn_feat,tst_feat])\n",
        "trn_emo_raw = pd.read_csv(file_emo_trn, header=None, usecols=[0])\n",
        "trn_emo_raw = trn_emo_raw.values\n",
        "# one-hot encoding of the classes\n",
        "trn_emo = []\n",
        "for label in trn_emo_raw:\n",
        "    if label == 0:\n",
        "        converted_label = [0,1,0] # medium\n",
        "    elif label == 1:\n",
        "        converted_label = [0,0,1] # high\n",
        "    else:\n",
        "        converted_label = [1,0,0] # low\n",
        "    trn_emo.append(converted_label)\n",
        "y_train = np.asarray(trn_emo)\n",
        "y_train_df = pd.DataFrame(y_train, index=None)\n",
        "tst_emo_raw = pd.read_csv(file_emo_tst, header=None, usecols=[0])\n",
        "tst_emo_raw = tst_emo_raw.values\n",
        "# one-hot encoding of the classes\n",
        "tst_emo = []\n",
        "for label in tst_emo_raw:\n",
        "    if label == 0:\n",
        "        converted_label = [1,0,0] # low\n",
        "    elif label == 1:\n",
        "        converted_label = [0,1,0] # medium\n",
        "    else:\n",
        "        converted_label = [0,0,1] # high\n",
        "    tst_emo.append(converted_label)\n",
        "y_test = np.asarray(tst_emo)\n",
        "y_test_df = pd.DataFrame(y_test, index=None)\n",
        "# combine the two databases\n",
        "all_emo_raw = pd.concat([y_train_df,y_test_df])\n",
        "\n",
        "# Grid search for best parameters\n",
        "para_list = []\n",
        "tst_pred_list = []\n",
        "f1_list = []\n",
        "count = 1\n",
        "for time_step in time_steps:\n",
        "    X = reshape_data(all_feat, time_step) # pad feature data\n",
        "    y = reshape_data(all_emo_raw, time_step) # pad label data\n",
        "    # save a subset of IEMOCAP for testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, shuffle=False)\n",
        "    for lstm_size in lstm_sizes:\n",
        "        for attention_width in attention_widths:\n",
        "            para_list.append([time_step, lstm_size, attention_width]) # save parameter set\n",
        "            print('\\n================================ No. %s of 27 ========================================' % count)\n",
        "            print('\\nParameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
        "                  % (time_step, lstm_size, attention_width))\n",
        "            # build model with given parameters\n",
        "            model = attBLSTM(lstm_size, attention_width, nb_class, opt_func)\n",
        "            # compile the model\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
        "            # training the model\n",
        "            model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, \n",
        "                      validation_split=0.05, callbacks=[early_stopping], verbose=0)\n",
        "            # evaluation\n",
        "            model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "            # save predictions\n",
        "            tst_pred = model.predict(X_test)\n",
        "            tst_pred_list.append(tst_pred) # save predictions\n",
        "\n",
        "            # print confusion matrix\n",
        "            y_test_non_category = [ np.argmax(t[0]) for t in y_test ]\n",
        "            y_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
        "            print('Confusion Matrix on test set')\n",
        "            print(confusion_matrix(y_test_non_category, y_predict_non_category))\n",
        "            tst_f1 = f1_score(y_test_non_category, y_predict_non_category, average='weighted')\n",
        "            f1_list.append(tst_f1) # save f1 score\n",
        "            print('Weighted F1-score on test set:', tst_f1)\n",
        "            # print grid search log\n",
        "            with open(file_log, 'a') as logfile:\n",
        "                logfile.write('\\n================================ No. %s of 27 ========================================' % count)\n",
        "                logfile.write('F1 = %s; Parameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
        "                              % (tst_f1, time_step, lstm_size, attention_width))\n",
        "                logfile.write('Confusion Matrix on test set')\n",
        "                logfile.write(confusion_matrix(y_test_non_category, y_predict_non_category))           \n",
        "            count = count + 1\n",
        "\n",
        "# save the best parameter set and its predictions\n",
        "best = f1_list.index(max(f1_list)) # find the highest F1 score\n",
        "result = f1_list[best]\n",
        "para = para_list[best]\n",
        "prediction = tst_pred_list[best]\n",
        "with open(file_pred, 'a') as predfile:\n",
        "    predfile.write('F1 = %s; Parameters: time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
        "                   % (result, para[0], para[1], para[2]))\n",
        "    for pred in prediction:\n",
        "        indi_pred = []\n",
        "        indi_pred = pred[0] # reform the seq prediction to individual samples\n",
        "        row = ', '.join(map(str, indi_pred))\n",
        "        predfile.write('%s\\n' % row)\n",
        "\n",
        "print('\\nDone!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.17.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=7c44e0880a2cfd73dbccbe5db850a21d5617a009e5884266d87558e5f65985c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.42.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "\n",
            "================================ No. 1 of 27 ========================================\n",
            "\n",
            "Parameters: time_step = 2, [h1, h2, h3] = [16, 8, 4], attention_width = 8\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
